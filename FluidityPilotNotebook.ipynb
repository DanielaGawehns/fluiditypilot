{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#to access file system\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# for extraction of Phi and other metrics\n",
    "from scipy.special import hyp2f1\n",
    "from scipy.optimize import fsolve\n",
    "from math import log\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Functions to create dataframe needed later in the process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust get_data.py to playgrounds:\n",
    "#def list_of_temporal_networks():\n",
    "    #very, very ugly way to get the number of .csv files in folder\n",
    "     #playgrounds=['kids_'+str(i) for i in range(int(len(os.listdir(path))/2)) ] \n",
    "    \n",
    "      #data_list=playgrounds\n",
    "    \n",
    "    #return data_list\n",
    "\n",
    "\n",
    "#setting parameters (??? Double check this!)\n",
    "\n",
    "def dataframe(time_series='Poisson'): #Q: why do I need to define a time_series here?\n",
    "#def dataframe():\n",
    "    #data=info[0:info.find('_')]\n",
    "    #version=info[1+info.find('_'):1+len(info)]\n",
    "    #species='Playground'\n",
    "    #interaction='Face-to-face'\n",
    "    delta_t=60*60*24 #not sure where delta is needed       \n",
    "    #day=int(version) #day is: 05/21/2013 unix: 1369094400\n",
    "    cols=['ID1','ID2','start_time','end_time']\n",
    "    \n",
    "    z=pd.read_csv(infile,sep='\\t',header=None,names=cols)\n",
    "    t_0=min(z['start_time']) #+ day*24*60*60 #set t0 as minimum start_time\n",
    "    t_end=t_0+24*60*60 #is this just set to 24 hrs after t_0??\n",
    "    #this is just to clean data of strange timestamps I??\n",
    "    df=z[(z.end_time<t_end)&(z.start_time>t_0)]\n",
    "    \n",
    "    phi_zero=0.3 #starting value -- play with parameter?\n",
    "\n",
    "#uncomment this to make all interactions 1 sec long! \n",
    "#(see comment in original code)\n",
    "    #df.drop(labels=['end_time'], axis=\"columns\", inplace=True)\n",
    "    \n",
    "#not sure why this was included in original code? \n",
    "#Some adjusting/preprocessing step?\n",
    "#    df['end_time']=[i+1 for i in df['start_time']]\n",
    "\n",
    "    return df,delta_t,phi_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to extract Phi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to extract Phi \n",
    "#copied verbatim from original codebase\n",
    "\n",
    "def nCr(n,r):\n",
    "    ans=1\n",
    "    while r>0:\n",
    "        ans=ans*(n-r+1)/r\n",
    "        r=r-1\n",
    "    return ans\n",
    "\n",
    "\n",
    "#This is equation (3) in the text\n",
    "def get_exp_degree(s,N,e,phi):\n",
    "\n",
    "    Q=(phi*(e**phi)*((1-e)**(s+1)))/((1-e**phi)*(s+1))\n",
    "    Psi=1-Q*hyp2f1( s+1, 1+phi, s+2, 1-e)   \n",
    "\n",
    "    return (N-1)*Psi\n",
    "    \n",
    "\n",
    "#to get phi, newtons method is employed and these error functions are needed:\n",
    "\n",
    "def get_error(k,s,N,e,phi):    \n",
    "   # print(k,s,N,e,phi)\n",
    "    \n",
    "    Exp_k=get_exp_degree(s,N,e,phi)\n",
    "\n",
    "    #error=((k-Exp_k)/min(s,N-1))**2\n",
    "    if s>1:\n",
    "        error=((k-Exp_k)**2)\n",
    "        #null_error=1+(min(s,N-1)-1)/2\n",
    "    else:\n",
    "        error=None\n",
    "        #null_error=None\n",
    "    return error#, null_error\n",
    "\n",
    "\n",
    "def get_total_error(phi,K,S):\n",
    "    C=(1-phi)/((N-1)*phi)\n",
    "    sol=fsolve(lambda x : (C+1)*(x**phi)-x-C, 0)\n",
    "    #print('C=',C)\n",
    "    e=sol[0] \n",
    "    \n",
    "    error_sum=0\n",
    " \n",
    "    for i in range(N):\n",
    "        if S[i]>1 and S[i]<160: \n",
    "            error=get_error(K[i],S[i],N,e,phi)\n",
    "            error_sum=error_sum+error\n",
    "\n",
    "    return error_sum\n",
    "\n",
    "#this function is only needed for goodness of fit measure \n",
    "def get_number_of_outliers(phi,K,S):\n",
    "    C=(1-phi)/((N-1)*phi)\n",
    "    sol=fsolve(lambda x : (C+1)*(x**phi)-x-C, 0)\n",
    "    #print('C=',C)\n",
    "    e=sol[0]\n",
    "\n",
    "    bigger_than_one=0    \n",
    "    for i in range(N):\n",
    "        if S[i]>1 and S[i]<160: \n",
    "            error=get_error(K[i],S[i],N,e,phi)\n",
    "            if error>2:\n",
    "                bigger_than_one=bigger_than_one+1        \n",
    "                \n",
    "    return bigger_than_one\n",
    "\n",
    "#def get_null_error(phi,K,S):\n",
    "#    C=(1-phi)/((N-1)*phi)\n",
    "#    sol=fsolve(lambda x : (C+1)*(x**phi)-x-C, 0)\n",
    "#    #print('C=',C)\n",
    "#    e=sol[0]\n",
    "#    \n",
    "#    error_sum=0    \n",
    "#    for i in range(N):\n",
    "#        if S[i]>1 and S[i]<160: \n",
    "#            error,null_error=get_error(K[i],S[i],N,e,phi)\n",
    "#            error_sum=error_sum+null_error\n",
    "#    return error_sum\n",
    "\n",
    "\n",
    "\n",
    "#should this be get_phi?\n",
    "#implements newton method to guess phi and \n",
    "#checks total error made in one go (function for error calculation above)\n",
    "\n",
    "def get_gamma(K,S,initial_guess):\n",
    "    #D is the degree sequence and T is the interactions sequence\n",
    "    global N\n",
    "    global rejected\n",
    "    rejected=0\n",
    "    N=len(K)\n",
    "    #print(N)\n",
    "    delta=0.01\n",
    "    # set intitial value for phi\n",
    "    phi=initial_guess\n",
    "    first_derivative=1\n",
    "    while abs(first_derivative)>0.01:\n",
    "        #print(phi,get_total_error(phi,K,S),first_derivative, second_derivative)\n",
    "        #implement newton method    \n",
    "        first_derivative=(get_total_error(phi+delta,K,S)- get_total_error(phi,K,S))/delta\n",
    "        second_derivative=(get_total_error(phi+2*delta,K,S)-2*get_total_error(phi+delta,K,S)+get_total_error(phi,K,S))/(delta**2)      \n",
    "        phi=phi-(first_derivative/second_derivative)\n",
    "\n",
    "    #print(phi,total_error(phi,K,S),first_derivative)  \n",
    "        \n",
    "    #get the corresponding value of e\n",
    "    C=(1-phi)/((N-1)*phi)\n",
    "    sol=fsolve(lambda x : (C+1)*(x**phi)-x-C, 0)\n",
    "    e=sol[0]\n",
    "    #goodness of fit GoF\n",
    "       \n",
    "    # fidelity=number of times error is larger than 1\n",
    "    \n",
    "    #GoF=((1/N)*(get_total_error(phi,K,S)))\n",
    "    \n",
    "    #GoF=(1/N)*get_number_of_outliers(phi,K,S)\n",
    "    \n",
    "    \n",
    "    #here the expected degree per node is stored\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(N):\n",
    "        if S[i]>1 and S[i]<160: \n",
    "            x.append(K[i])\n",
    "            y.append(get_exp_degree(S[i],N,e,phi))\n",
    "            \n",
    "    r,p=pearsonr(x,y)\n",
    "    \n",
    "    return phi,e#,r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Degrees and Strengths and Phi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/daniela/fluiditypilot/analysis/data/derived_data/*2_40_1_.txt\n",
      "current file is:/Users/daniela/fluiditypilot/analysis/data/derived_data/PilotBeagle_2_40_1_.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.0/libexec/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "#adjusted get_degrees:\n",
    "\n",
    "## empty lists and vectors to store info:\n",
    "excess_degree_values={}\n",
    "degree_values={}\n",
    "mean_strength_values={}\n",
    "population_values={}\n",
    "\n",
    "# empty list to store group level information\n",
    "#data_list= list_of_temporal_networks() \n",
    "\n",
    "Degrees = []\n",
    "Strengths = []\n",
    "NetworkSize = []\n",
    "MeanDegrees = []\n",
    "MeanStrength = []\n",
    "Phi = []\n",
    "Epsilon = []\n",
    "excess_degree_values = []\n",
    "Heterogeneity_Weights = []\n",
    "ExpDegrees = []\n",
    "IDlists = []\n",
    "\n",
    "\n",
    "#set here the path to the data as transformed by pipeline in R\n",
    "#path = \n",
    "\n",
    "#set here which parameter combination to analyse:\n",
    "paramCombi = '2_40_1'\n",
    "\n",
    "focusOn = '*2_40_1_.txt'\n",
    "\n",
    "store_filename=[] \n",
    "\n",
    "print(os.path.join(path, focusOn) ) \n",
    "\n",
    "for infile in glob.glob(os.path.join(path, focusOn) ):\n",
    "    print ('current file is:' + infile)\n",
    "    #add here a record of which files are processed:\n",
    "    store_filename.append(infile)\n",
    "    \n",
    "    #get all info needed to proceed:\n",
    "    df,delta_t,phi_zero = dataframe (infile) #adjust dataframe()\n",
    "    \n",
    "    ############################################################\n",
    "    \n",
    "    ID_list=list(set(pd.concat([df['ID1'],df['ID2']])))\n",
    "\n",
    "    N=len(ID_list)          \n",
    "    #population_values[data]=N\n",
    "    \n",
    "    #################################################################### \n",
    "    \n",
    "    K=[len(set(pd.concat([df[df['ID1']==ID]['ID2'],df[df['ID2']==ID]['ID1']]))) for ID in ID_list]\n",
    "    S=[len(pd.concat([df[df['ID1']==ID]['ID2'],df[df['ID2']==ID]['ID1']])) for ID in ID_list]  \n",
    "\n",
    "    ####################################################################\n",
    "    \n",
    "    W=[]\n",
    "    edge_list=[]    \n",
    "    for i,row in df.iterrows():\n",
    "        edge=tuple(sorted([row['ID1'],row['ID2']]))\n",
    "        if edge not in edge_list:\n",
    "            edge_list.append(edge)\n",
    "            weight=len(df[(df['ID1']==edge[0])&(df['ID2']==edge[1])])+len(df[(df['ID2']==edge[0])&(df['ID1']==edge[1])])\n",
    "            W.append(weight)\n",
    "    \n",
    "    #mean weight\n",
    "    mean_weight=np.mean(W)\n",
    "    #Weight heterogeneity\n",
    "    weight_heterogeneity=np.var(W)/np.mean(W)\n",
    "    \n",
    "    ###################################################################\n",
    "    M = get_gamma(K,S,phi_zero) #phi_zero from parameter settings is the initial \"guess\"\n",
    "    phi = M[0]\n",
    "    epsilon = M[1]\n",
    "    #fidelity=M[2]\n",
    "    #print(data,'MSE:',phi)\n",
    "\n",
    "\n",
    "    mean_degree=np.mean(K)\n",
    "    var_degree=np.var(K)\n",
    "    \n",
    "    mean_strength=np.mean(S)#/delta_t ### This is where delta_t comes into play!!\n",
    "    var_strength=np.var(S)\n",
    "    \n",
    "    #here the expected degree per node is stored\n",
    "    C=(1-phi)/((N-1)*phi)\n",
    "    sol=fsolve(lambda x : (C+1)*(x**phi)-x-C, 0)\n",
    "    e=sol[0]\n",
    "    #x=[]\n",
    "    y=[]\n",
    "    for i in range(N):\n",
    "        if S[i]>1 and S[i]<160: \n",
    "            #x.append(K[i])\n",
    "            y.append(get_exp_degree(S[i],N,e,phi))\n",
    "    \n",
    "    \n",
    "    #store networklevel - information\n",
    "    NetworkSize.append(N)\n",
    "    MeanDegrees.append(np.mean(K))\n",
    "    MeanStrength.append(np.mean(S))\n",
    "    excess_degree_values.append(mean_degree + (var_degree/mean_degree))\n",
    "    Phi.append(phi)\n",
    "    Epsilon.append(epsilon)\n",
    "    Degrees.append(K)\n",
    "    Strengths.append(S)\n",
    "    IDlists.append (ID_list)\n",
    "    Heterogeneity_Weights.append(weight_heterogeneity)\n",
    "    ExpDegrees.append(y)\n",
    "    \n",
    "    \n",
    "# dirList=os.listdir(path)\n",
    "\n",
    "# print(path)\n",
    "\n",
    "#store results:\n",
    "\n",
    "data = pd.DataFrame([Phi, Epsilon, NetworkSize, MeanDegrees, MeanStrength, \n",
    "                     excess_degree_values,Heterogeneity_Weights,store_filename])\n",
    "data.to_csv(\"resultsPilot\"+paramCombi+\".csv\",index=False, sep='\\t')\n",
    "\n",
    "Degrees_out = pd.DataFrame([Degrees])\n",
    "Degrees_out.to_csv(\"degreesPilot\"+paramCombi+\".csv\",index=False, sep='\\t')\n",
    "\n",
    "Strengths_out = pd.DataFrame([Strengths])\n",
    "Strengths_out.to_csv(\"strengthPilot\"+paramCombi+\".csv\",index=False, sep='\\t')\n",
    "\n",
    "ExpDegrees_out = pd.DataFrame([ExpDegrees])\n",
    "ExpDegrees_out.to_csv(\"ExpDegreesPilot\"+paramCombi+\".csv\",index=False, sep='\\t')\n",
    "\n",
    "ID_list_out = pd.DataFrame([IDlists])\n",
    "ID_list_out.to_csv(\"IDListPilot\"+paramCombi+\".csv\",index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
